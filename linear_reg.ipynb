{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.],\n",
       "         [102.,  43.,  37.],\n",
       "         [ 69.,  96.,  70.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9862,  0.4953,  0.6018],\n",
       "         [ 0.1656, -0.7588,  0.2720]], requires_grad=True),\n",
       " tensor([-1.0714, -0.7786], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(2,3,requires_grad=True)\n",
    "b = torch.randn(2,requires_grad=True)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x@w.t()+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[129.9805, -27.8386],\n",
       "         [170.7705, -35.0820],\n",
       "         [185.9969, -72.2822],\n",
       "         [143.0825,  -6.4579],\n",
       "         [156.6476, -43.1627]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(t1,t2):\n",
    "    diff = t1-t2\n",
    "    return (torch.sum(diff*diff))/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(13397.6299, grad_fn=<DivBackward0>),\n",
       " tensor(115.7481, grad_fn=<SqrtBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds,targets)\n",
    "l = torch.sqrt(loss)\n",
    "loss,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9862,  0.4953,  0.6018],\n",
       "         [ 0.1656, -0.7588,  0.2720]], requires_grad=True),\n",
       " tensor([[  7090.1055,   6438.1616,   4209.5361],\n",
       "         [-10601.4336, -12694.9043,  -7556.4014]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0714, -0.7786], requires_grad=True),\n",
       " tensor([  81.0956, -128.9647]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b,b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7090.1055,   6438.1616,   4209.5361],\n",
       "        [-10601.4336, -12694.9043,  -7556.4014]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13397.6299, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds,targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "w.grad,b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[118.6803,  -8.3435],\n",
       "         [155.9580,  -9.4258],\n",
       "         [168.7591, -41.6637],\n",
       "         [131.5238,  12.6115],\n",
       "         [142.6273, -18.3698]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9389.3516, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds,targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  5926.3184,   5195.4692,   3440.9768],\n",
       "         [ -8585.9424, -10521.5264,  -6216.9561]]),\n",
       " tensor([  67.3097, -105.0383]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad,b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad* 1e-5\n",
    "    b -= b.grad* 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8560,  0.3789,  0.5253],\n",
       "         [ 0.3574, -0.5267,  0.4097]], requires_grad=True),\n",
       " tensor([-1.0728, -0.7763], requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[109.3928,   7.6480],\n",
       "         [143.7902,  11.6263],\n",
       "         [154.6448, -16.4883],\n",
       "         [121.9711,  28.1948],\n",
       "         [131.1412,   2.0081]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6683.6904, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds,targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds,targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4927, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 56.9840,  70.1767],\n",
       "         [ 82.3205, 100.7532],\n",
       "         [118.7227, 132.9801],\n",
       "         [ 21.1044,  37.0318],\n",
       "         [101.8912, 119.1167]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using built in functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.],\n",
       "         [102.,  43.,  37.],\n",
       "         [ 69.,  96.,  70.],\n",
       "         [ 74.,  66.,  43.],\n",
       "         [ 91.,  87.,  65.],\n",
       "         [ 88., 134.,  59.],\n",
       "         [101.,  44.,  37.],\n",
       "         [ 68.,  96.,  71.],\n",
       "         [ 73.,  66.,  44.],\n",
       "         [ 92.,  87.,  64.],\n",
       "         [ 87., 135.,  57.],\n",
       "         [103.,  43.,  36.],\n",
       "         [ 68.,  97.,  70.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.],\n",
       "         [ 57.,  69.],\n",
       "         [ 80., 102.],\n",
       "         [118., 132.],\n",
       "         [ 21.,  38.],\n",
       "         [104., 118.],\n",
       "         [ 57.,  69.],\n",
       "         [ 82., 100.],\n",
       "         [118., 134.],\n",
       "         [ 20.,  38.],\n",
       "         [102., 120.]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(inputs,targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5\n",
    "train_dl=DataLoader(train_ds,bs,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 68.,  96.,  71.],\n",
      "        [ 74.,  66.,  43.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [101.,  44.,  37.],\n",
      "        [ 73.,  66.,  44.]]) tensor([[104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [118., 134.],\n",
      "        [ 21.,  38.],\n",
      "        [ 57.,  69.]])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(xb,yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1611,  0.0576, -0.3304],\n",
      "        [ 0.5219,  0.4459, -0.0694]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.2304,  0.3230], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(3,2)\n",
    "print(model.weight,model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1611,  0.0576, -0.3304],\n",
       "         [ 0.5219,  0.4459, -0.0694]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2304,  0.3230], requires_grad=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1.1853,  65.3137],\n",
       "         [ -1.6425,  82.6146],\n",
       "         [  2.3454, 101.4529],\n",
       "         [  6.4574,  70.1655],\n",
       "         [ -6.7086,  74.2826],\n",
       "         [  1.2888,  65.3898],\n",
       "         [ -2.0305,  82.0993],\n",
       "         [  2.1761, 101.9054],\n",
       "         [  6.3539,  70.0894],\n",
       "         [ -7.2001,  73.6913],\n",
       "         [  0.7973,  64.7984],\n",
       "         [ -1.5390,  82.6907],\n",
       "         [  2.7334, 101.9681],\n",
       "         [  6.9489,  70.7568],\n",
       "         [ -6.8121,  74.2065]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.],\n",
       "         [ 57.,  69.],\n",
       "         [ 80., 102.],\n",
       "         [118., 132.],\n",
       "         [ 21.,  38.],\n",
       "         [104., 118.],\n",
       "         [ 57.,  69.],\n",
       "         [ 82., 100.],\n",
       "         [118., 134.],\n",
       "         [ 20.,  38.],\n",
       "         [102., 120.]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4008.6128, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(preds,targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(),lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred,yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        if (epoch+1)%10 ==0:\n",
    "             print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.1520\n",
      "Epoch [20/100], Loss: 0.8055\n",
      "Epoch [30/100], Loss: 0.6948\n",
      "Epoch [40/100], Loss: 1.0334\n",
      "Epoch [50/100], Loss: 0.9246\n",
      "Epoch [60/100], Loss: 0.8367\n",
      "Epoch [70/100], Loss: 1.2355\n",
      "Epoch [80/100], Loss: 0.9778\n",
      "Epoch [90/100], Loss: 0.9297\n",
      "Epoch [100/100], Loss: 1.0282\n"
     ]
    }
   ],
   "source": [
    "fit(100,model,loss_fn,opt,train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 1.3296\n",
      "Epoch [20/10000], Loss: 1.5479\n",
      "Epoch [30/10000], Loss: 0.8673\n",
      "Epoch [40/10000], Loss: 0.9240\n",
      "Epoch [50/10000], Loss: 0.9201\n",
      "Epoch [60/10000], Loss: 0.8628\n",
      "Epoch [70/10000], Loss: 1.4985\n",
      "Epoch [80/10000], Loss: 1.1915\n",
      "Epoch [90/10000], Loss: 0.9621\n",
      "Epoch [100/10000], Loss: 0.9337\n",
      "Epoch [110/10000], Loss: 1.2354\n",
      "Epoch [120/10000], Loss: 0.8052\n",
      "Epoch [130/10000], Loss: 0.9724\n",
      "Epoch [140/10000], Loss: 1.0729\n",
      "Epoch [150/10000], Loss: 1.3827\n",
      "Epoch [160/10000], Loss: 1.0105\n",
      "Epoch [170/10000], Loss: 1.2341\n",
      "Epoch [180/10000], Loss: 1.0196\n",
      "Epoch [190/10000], Loss: 1.4795\n",
      "Epoch [200/10000], Loss: 0.8822\n",
      "Epoch [210/10000], Loss: 1.4558\n",
      "Epoch [220/10000], Loss: 0.6628\n",
      "Epoch [230/10000], Loss: 0.7857\n",
      "Epoch [240/10000], Loss: 0.8246\n",
      "Epoch [250/10000], Loss: 0.8884\n",
      "Epoch [260/10000], Loss: 1.0683\n",
      "Epoch [270/10000], Loss: 0.9470\n",
      "Epoch [280/10000], Loss: 0.7415\n",
      "Epoch [290/10000], Loss: 1.1276\n",
      "Epoch [300/10000], Loss: 0.6806\n",
      "Epoch [310/10000], Loss: 1.0661\n",
      "Epoch [320/10000], Loss: 0.9432\n",
      "Epoch [330/10000], Loss: 0.8427\n",
      "Epoch [340/10000], Loss: 0.7910\n",
      "Epoch [350/10000], Loss: 1.2852\n",
      "Epoch [360/10000], Loss: 0.8836\n",
      "Epoch [370/10000], Loss: 1.3556\n",
      "Epoch [380/10000], Loss: 0.9864\n",
      "Epoch [390/10000], Loss: 1.5319\n",
      "Epoch [400/10000], Loss: 1.1888\n",
      "Epoch [410/10000], Loss: 1.2276\n",
      "Epoch [420/10000], Loss: 0.8691\n",
      "Epoch [430/10000], Loss: 1.0299\n",
      "Epoch [440/10000], Loss: 1.1532\n",
      "Epoch [450/10000], Loss: 0.8317\n",
      "Epoch [460/10000], Loss: 1.0054\n",
      "Epoch [470/10000], Loss: 0.7017\n",
      "Epoch [480/10000], Loss: 1.2921\n",
      "Epoch [490/10000], Loss: 0.7404\n",
      "Epoch [500/10000], Loss: 0.9166\n",
      "Epoch [510/10000], Loss: 1.4994\n",
      "Epoch [520/10000], Loss: 1.0266\n",
      "Epoch [530/10000], Loss: 0.8724\n",
      "Epoch [540/10000], Loss: 0.9269\n",
      "Epoch [550/10000], Loss: 0.9220\n",
      "Epoch [560/10000], Loss: 1.0533\n",
      "Epoch [570/10000], Loss: 0.8206\n",
      "Epoch [580/10000], Loss: 1.3755\n",
      "Epoch [590/10000], Loss: 0.9623\n",
      "Epoch [600/10000], Loss: 0.7799\n",
      "Epoch [610/10000], Loss: 0.9053\n",
      "Epoch [620/10000], Loss: 1.2385\n",
      "Epoch [630/10000], Loss: 0.8212\n",
      "Epoch [640/10000], Loss: 0.9494\n",
      "Epoch [650/10000], Loss: 0.6758\n",
      "Epoch [660/10000], Loss: 1.0640\n",
      "Epoch [670/10000], Loss: 1.3559\n",
      "Epoch [680/10000], Loss: 0.9472\n",
      "Epoch [690/10000], Loss: 0.9202\n",
      "Epoch [700/10000], Loss: 0.9334\n",
      "Epoch [710/10000], Loss: 1.3577\n",
      "Epoch [720/10000], Loss: 0.9463\n",
      "Epoch [730/10000], Loss: 1.0502\n",
      "Epoch [740/10000], Loss: 0.9372\n",
      "Epoch [750/10000], Loss: 0.8705\n",
      "Epoch [760/10000], Loss: 1.3451\n",
      "Epoch [770/10000], Loss: 0.8047\n",
      "Epoch [780/10000], Loss: 1.3469\n",
      "Epoch [790/10000], Loss: 1.2943\n",
      "Epoch [800/10000], Loss: 1.0718\n",
      "Epoch [810/10000], Loss: 1.1869\n",
      "Epoch [820/10000], Loss: 1.3095\n",
      "Epoch [830/10000], Loss: 1.3294\n",
      "Epoch [840/10000], Loss: 1.0278\n",
      "Epoch [850/10000], Loss: 0.9423\n",
      "Epoch [860/10000], Loss: 1.2279\n",
      "Epoch [870/10000], Loss: 1.1960\n",
      "Epoch [880/10000], Loss: 0.9907\n",
      "Epoch [890/10000], Loss: 1.0495\n",
      "Epoch [900/10000], Loss: 0.9475\n",
      "Epoch [910/10000], Loss: 1.0525\n",
      "Epoch [920/10000], Loss: 0.9169\n",
      "Epoch [930/10000], Loss: 1.0099\n",
      "Epoch [940/10000], Loss: 1.1869\n",
      "Epoch [950/10000], Loss: 0.8329\n",
      "Epoch [960/10000], Loss: 0.7465\n",
      "Epoch [970/10000], Loss: 1.0396\n",
      "Epoch [980/10000], Loss: 1.4148\n",
      "Epoch [990/10000], Loss: 1.1198\n",
      "Epoch [1000/10000], Loss: 0.8070\n",
      "Epoch [1010/10000], Loss: 0.9915\n",
      "Epoch [1020/10000], Loss: 0.8398\n",
      "Epoch [1030/10000], Loss: 1.4795\n",
      "Epoch [1040/10000], Loss: 1.1978\n",
      "Epoch [1050/10000], Loss: 1.0731\n",
      "Epoch [1060/10000], Loss: 1.5445\n",
      "Epoch [1070/10000], Loss: 0.9547\n",
      "Epoch [1080/10000], Loss: 1.1128\n",
      "Epoch [1090/10000], Loss: 0.7827\n",
      "Epoch [1100/10000], Loss: 0.7575\n",
      "Epoch [1110/10000], Loss: 0.7809\n",
      "Epoch [1120/10000], Loss: 1.2691\n",
      "Epoch [1130/10000], Loss: 0.8573\n",
      "Epoch [1140/10000], Loss: 0.7312\n",
      "Epoch [1150/10000], Loss: 1.3119\n",
      "Epoch [1160/10000], Loss: 0.7941\n",
      "Epoch [1170/10000], Loss: 1.0850\n",
      "Epoch [1180/10000], Loss: 1.0716\n",
      "Epoch [1190/10000], Loss: 1.1191\n",
      "Epoch [1200/10000], Loss: 1.3032\n",
      "Epoch [1210/10000], Loss: 0.7131\n",
      "Epoch [1220/10000], Loss: 0.8191\n",
      "Epoch [1230/10000], Loss: 0.8526\n",
      "Epoch [1240/10000], Loss: 1.5104\n",
      "Epoch [1250/10000], Loss: 1.1978\n",
      "Epoch [1260/10000], Loss: 0.8950\n",
      "Epoch [1270/10000], Loss: 0.6772\n",
      "Epoch [1280/10000], Loss: 1.2942\n",
      "Epoch [1290/10000], Loss: 0.8499\n",
      "Epoch [1300/10000], Loss: 1.1610\n",
      "Epoch [1310/10000], Loss: 0.9383\n",
      "Epoch [1320/10000], Loss: 1.2983\n",
      "Epoch [1330/10000], Loss: 0.8870\n",
      "Epoch [1340/10000], Loss: 0.8579\n",
      "Epoch [1350/10000], Loss: 1.1532\n",
      "Epoch [1360/10000], Loss: 0.8836\n",
      "Epoch [1370/10000], Loss: 0.7330\n",
      "Epoch [1380/10000], Loss: 1.2691\n",
      "Epoch [1390/10000], Loss: 1.1598\n",
      "Epoch [1400/10000], Loss: 0.9816\n",
      "Epoch [1410/10000], Loss: 0.7575\n",
      "Epoch [1420/10000], Loss: 0.7718\n",
      "Epoch [1430/10000], Loss: 0.9558\n",
      "Epoch [1440/10000], Loss: 0.7564\n",
      "Epoch [1450/10000], Loss: 1.2709\n",
      "Epoch [1460/10000], Loss: 0.9818\n",
      "Epoch [1470/10000], Loss: 1.3207\n",
      "Epoch [1480/10000], Loss: 1.1435\n",
      "Epoch [1490/10000], Loss: 0.9284\n",
      "Epoch [1500/10000], Loss: 0.9131\n",
      "Epoch [1510/10000], Loss: 0.8832\n",
      "Epoch [1520/10000], Loss: 1.1836\n",
      "Epoch [1530/10000], Loss: 1.0396\n",
      "Epoch [1540/10000], Loss: 1.1719\n",
      "Epoch [1550/10000], Loss: 1.1093\n",
      "Epoch [1560/10000], Loss: 0.7971\n",
      "Epoch [1570/10000], Loss: 1.2624\n",
      "Epoch [1580/10000], Loss: 1.1815\n",
      "Epoch [1590/10000], Loss: 1.2113\n",
      "Epoch [1600/10000], Loss: 0.7984\n",
      "Epoch [1610/10000], Loss: 0.8076\n",
      "Epoch [1620/10000], Loss: 1.0875\n",
      "Epoch [1630/10000], Loss: 1.2747\n",
      "Epoch [1640/10000], Loss: 1.2536\n",
      "Epoch [1650/10000], Loss: 1.2583\n",
      "Epoch [1660/10000], Loss: 0.9338\n",
      "Epoch [1670/10000], Loss: 1.1483\n",
      "Epoch [1680/10000], Loss: 0.9613\n",
      "Epoch [1690/10000], Loss: 1.1848\n",
      "Epoch [1700/10000], Loss: 0.9493\n",
      "Epoch [1710/10000], Loss: 0.8300\n",
      "Epoch [1720/10000], Loss: 1.2640\n",
      "Epoch [1730/10000], Loss: 0.8472\n",
      "Epoch [1740/10000], Loss: 1.3893\n",
      "Epoch [1750/10000], Loss: 1.3931\n",
      "Epoch [1760/10000], Loss: 1.2747\n",
      "Epoch [1770/10000], Loss: 1.0024\n",
      "Epoch [1780/10000], Loss: 0.8541\n",
      "Epoch [1790/10000], Loss: 1.1045\n",
      "Epoch [1800/10000], Loss: 0.8198\n",
      "Epoch [1810/10000], Loss: 1.0276\n",
      "Epoch [1820/10000], Loss: 1.2966\n",
      "Epoch [1830/10000], Loss: 0.9583\n",
      "Epoch [1840/10000], Loss: 0.8807\n",
      "Epoch [1850/10000], Loss: 1.0322\n",
      "Epoch [1860/10000], Loss: 0.7195\n",
      "Epoch [1870/10000], Loss: 0.9012\n",
      "Epoch [1880/10000], Loss: 1.2345\n",
      "Epoch [1890/10000], Loss: 0.9332\n",
      "Epoch [1900/10000], Loss: 1.3536\n",
      "Epoch [1910/10000], Loss: 0.7754\n",
      "Epoch [1920/10000], Loss: 0.7485\n",
      "Epoch [1930/10000], Loss: 0.7391\n",
      "Epoch [1940/10000], Loss: 1.4774\n",
      "Epoch [1950/10000], Loss: 1.2506\n",
      "Epoch [1960/10000], Loss: 0.9386\n",
      "Epoch [1970/10000], Loss: 1.3228\n",
      "Epoch [1980/10000], Loss: 0.8698\n",
      "Epoch [1990/10000], Loss: 1.0673\n",
      "Epoch [2000/10000], Loss: 1.3539\n",
      "Epoch [2010/10000], Loss: 0.8377\n",
      "Epoch [2020/10000], Loss: 1.1683\n",
      "Epoch [2030/10000], Loss: 0.8711\n",
      "Epoch [2040/10000], Loss: 0.8975\n",
      "Epoch [2050/10000], Loss: 1.2920\n",
      "Epoch [2060/10000], Loss: 1.2241\n",
      "Epoch [2070/10000], Loss: 1.2569\n",
      "Epoch [2080/10000], Loss: 0.8007\n",
      "Epoch [2090/10000], Loss: 1.5012\n",
      "Epoch [2100/10000], Loss: 1.2541\n",
      "Epoch [2110/10000], Loss: 1.2362\n",
      "Epoch [2120/10000], Loss: 0.8499\n",
      "Epoch [2130/10000], Loss: 1.1536\n",
      "Epoch [2140/10000], Loss: 1.2632\n",
      "Epoch [2150/10000], Loss: 0.7935\n",
      "Epoch [2160/10000], Loss: 0.9621\n",
      "Epoch [2170/10000], Loss: 0.8134\n",
      "Epoch [2180/10000], Loss: 0.8606\n",
      "Epoch [2190/10000], Loss: 0.9289\n",
      "Epoch [2200/10000], Loss: 1.3053\n",
      "Epoch [2210/10000], Loss: 1.5449\n",
      "Epoch [2220/10000], Loss: 1.2439\n",
      "Epoch [2230/10000], Loss: 0.9441\n",
      "Epoch [2240/10000], Loss: 1.1091\n",
      "Epoch [2250/10000], Loss: 1.3357\n",
      "Epoch [2260/10000], Loss: 1.3031\n",
      "Epoch [2270/10000], Loss: 1.4554\n",
      "Epoch [2280/10000], Loss: 1.3053\n",
      "Epoch [2290/10000], Loss: 1.0818\n",
      "Epoch [2300/10000], Loss: 0.8052\n",
      "Epoch [2310/10000], Loss: 0.7370\n",
      "Epoch [2320/10000], Loss: 1.2954\n",
      "Epoch [2330/10000], Loss: 1.0527\n",
      "Epoch [2340/10000], Loss: 0.8080\n",
      "Epoch [2350/10000], Loss: 0.8427\n",
      "Epoch [2360/10000], Loss: 0.9666\n",
      "Epoch [2370/10000], Loss: 1.1143\n",
      "Epoch [2380/10000], Loss: 0.9994\n",
      "Epoch [2390/10000], Loss: 1.1966\n",
      "Epoch [2400/10000], Loss: 1.0289\n",
      "Epoch [2410/10000], Loss: 0.8398\n",
      "Epoch [2420/10000], Loss: 0.7818\n",
      "Epoch [2430/10000], Loss: 1.3161\n",
      "Epoch [2440/10000], Loss: 0.7893\n",
      "Epoch [2450/10000], Loss: 1.1073\n",
      "Epoch [2460/10000], Loss: 0.8453\n",
      "Epoch [2470/10000], Loss: 1.3406\n",
      "Epoch [2480/10000], Loss: 0.6872\n",
      "Epoch [2490/10000], Loss: 1.1137\n",
      "Epoch [2500/10000], Loss: 0.8588\n",
      "Epoch [2510/10000], Loss: 0.6432\n",
      "Epoch [2520/10000], Loss: 1.1357\n",
      "Epoch [2530/10000], Loss: 1.1719\n",
      "Epoch [2540/10000], Loss: 0.7805\n",
      "Epoch [2550/10000], Loss: 0.7601\n",
      "Epoch [2560/10000], Loss: 0.9208\n",
      "Epoch [2570/10000], Loss: 0.7641\n",
      "Epoch [2580/10000], Loss: 1.3191\n",
      "Epoch [2590/10000], Loss: 0.7498\n",
      "Epoch [2600/10000], Loss: 0.9582\n",
      "Epoch [2610/10000], Loss: 0.9269\n",
      "Epoch [2620/10000], Loss: 1.3777\n",
      "Epoch [2630/10000], Loss: 0.8411\n",
      "Epoch [2640/10000], Loss: 0.8230\n",
      "Epoch [2650/10000], Loss: 1.3469\n",
      "Epoch [2660/10000], Loss: 1.4378\n",
      "Epoch [2670/10000], Loss: 0.8234\n",
      "Epoch [2680/10000], Loss: 1.1978\n",
      "Epoch [2690/10000], Loss: 0.8047\n",
      "Epoch [2700/10000], Loss: 0.9205\n",
      "Epoch [2710/10000], Loss: 1.1996\n",
      "Epoch [2720/10000], Loss: 0.7972\n",
      "Epoch [2730/10000], Loss: 1.1701\n",
      "Epoch [2740/10000], Loss: 0.8678\n",
      "Epoch [2750/10000], Loss: 0.6803\n",
      "Epoch [2760/10000], Loss: 0.9956\n",
      "Epoch [2770/10000], Loss: 0.8266\n",
      "Epoch [2780/10000], Loss: 0.9401\n",
      "Epoch [2790/10000], Loss: 1.0820\n",
      "Epoch [2800/10000], Loss: 0.7086\n",
      "Epoch [2810/10000], Loss: 1.1719\n",
      "Epoch [2820/10000], Loss: 1.2836\n",
      "Epoch [2830/10000], Loss: 1.1032\n",
      "Epoch [2840/10000], Loss: 0.9523\n",
      "Epoch [2850/10000], Loss: 0.9909\n",
      "Epoch [2860/10000], Loss: 0.8759\n",
      "Epoch [2870/10000], Loss: 0.8347\n",
      "Epoch [2880/10000], Loss: 0.7464\n",
      "Epoch [2890/10000], Loss: 1.4925\n",
      "Epoch [2900/10000], Loss: 0.6993\n",
      "Epoch [2910/10000], Loss: 1.1717\n",
      "Epoch [2920/10000], Loss: 0.8002\n",
      "Epoch [2930/10000], Loss: 0.7788\n",
      "Epoch [2940/10000], Loss: 1.2962\n",
      "Epoch [2950/10000], Loss: 0.6933\n",
      "Epoch [2960/10000], Loss: 1.0159\n",
      "Epoch [2970/10000], Loss: 1.2852\n",
      "Epoch [2980/10000], Loss: 1.2293\n",
      "Epoch [2990/10000], Loss: 1.2671\n",
      "Epoch [3000/10000], Loss: 1.0698\n",
      "Epoch [3010/10000], Loss: 1.2144\n",
      "Epoch [3020/10000], Loss: 1.5172\n",
      "Epoch [3030/10000], Loss: 0.9990\n",
      "Epoch [3040/10000], Loss: 1.0212\n",
      "Epoch [3050/10000], Loss: 0.8001\n",
      "Epoch [3060/10000], Loss: 0.9582\n",
      "Epoch [3070/10000], Loss: 0.8765\n",
      "Epoch [3080/10000], Loss: 1.2367\n",
      "Epoch [3090/10000], Loss: 0.7724\n",
      "Epoch [3100/10000], Loss: 0.8495\n",
      "Epoch [3110/10000], Loss: 1.4860\n",
      "Epoch [3120/10000], Loss: 0.8666\n",
      "Epoch [3130/10000], Loss: 0.7132\n",
      "Epoch [3140/10000], Loss: 1.1553\n",
      "Epoch [3150/10000], Loss: 0.8765\n",
      "Epoch [3160/10000], Loss: 1.2065\n",
      "Epoch [3170/10000], Loss: 1.2941\n",
      "Epoch [3180/10000], Loss: 1.3098\n",
      "Epoch [3190/10000], Loss: 1.2420\n",
      "Epoch [3200/10000], Loss: 1.1083\n",
      "Epoch [3210/10000], Loss: 0.9205\n",
      "Epoch [3220/10000], Loss: 1.2038\n",
      "Epoch [3230/10000], Loss: 1.1390\n",
      "Epoch [3240/10000], Loss: 1.4565\n",
      "Epoch [3250/10000], Loss: 1.0485\n",
      "Epoch [3260/10000], Loss: 0.7676\n",
      "Epoch [3270/10000], Loss: 1.2099\n",
      "Epoch [3280/10000], Loss: 0.9600\n",
      "Epoch [3290/10000], Loss: 0.8546\n",
      "Epoch [3300/10000], Loss: 0.7993\n",
      "Epoch [3310/10000], Loss: 1.2533\n",
      "Epoch [3320/10000], Loss: 0.7561\n",
      "Epoch [3330/10000], Loss: 0.8445\n",
      "Epoch [3340/10000], Loss: 1.4786\n",
      "Epoch [3350/10000], Loss: 1.2571\n",
      "Epoch [3360/10000], Loss: 0.8218\n",
      "Epoch [3370/10000], Loss: 1.0895\n",
      "Epoch [3380/10000], Loss: 0.7047\n",
      "Epoch [3390/10000], Loss: 0.7705\n",
      "Epoch [3400/10000], Loss: 1.0464\n",
      "Epoch [3410/10000], Loss: 0.7190\n",
      "Epoch [3420/10000], Loss: 0.9805\n",
      "Epoch [3430/10000], Loss: 0.8269\n",
      "Epoch [3440/10000], Loss: 0.7830\n",
      "Epoch [3450/10000], Loss: 0.7974\n",
      "Epoch [3460/10000], Loss: 1.4943\n",
      "Epoch [3470/10000], Loss: 0.8956\n",
      "Epoch [3480/10000], Loss: 1.2072\n",
      "Epoch [3490/10000], Loss: 1.3137\n",
      "Epoch [3500/10000], Loss: 0.9453\n",
      "Epoch [3510/10000], Loss: 0.9876\n",
      "Epoch [3520/10000], Loss: 0.8871\n",
      "Epoch [3530/10000], Loss: 0.8383\n",
      "Epoch [3540/10000], Loss: 0.9169\n",
      "Epoch [3550/10000], Loss: 0.8052\n",
      "Epoch [3560/10000], Loss: 0.9344\n",
      "Epoch [3570/10000], Loss: 0.9505\n",
      "Epoch [3580/10000], Loss: 0.8230\n",
      "Epoch [3590/10000], Loss: 0.6956\n",
      "Epoch [3600/10000], Loss: 0.9028\n",
      "Epoch [3610/10000], Loss: 0.8482\n",
      "Epoch [3620/10000], Loss: 1.1191\n",
      "Epoch [3630/10000], Loss: 0.8459\n",
      "Epoch [3640/10000], Loss: 1.1271\n",
      "Epoch [3650/10000], Loss: 0.9522\n",
      "Epoch [3660/10000], Loss: 1.2420\n",
      "Epoch [3670/10000], Loss: 0.8698\n",
      "Epoch [3680/10000], Loss: 0.9990\n",
      "Epoch [3690/10000], Loss: 1.2472\n",
      "Epoch [3700/10000], Loss: 1.4990\n",
      "Epoch [3710/10000], Loss: 1.0485\n",
      "Epoch [3720/10000], Loss: 0.8536\n",
      "Epoch [3730/10000], Loss: 0.9513\n",
      "Epoch [3740/10000], Loss: 1.3117\n",
      "Epoch [3750/10000], Loss: 0.7986\n",
      "Epoch [3760/10000], Loss: 1.0255\n",
      "Epoch [3770/10000], Loss: 0.8830\n",
      "Epoch [3780/10000], Loss: 1.0950\n",
      "Epoch [3790/10000], Loss: 1.4469\n",
      "Epoch [3800/10000], Loss: 1.0266\n",
      "Epoch [3810/10000], Loss: 0.8874\n",
      "Epoch [3820/10000], Loss: 1.0726\n",
      "Epoch [3830/10000], Loss: 1.0089\n",
      "Epoch [3840/10000], Loss: 1.5826\n",
      "Epoch [3850/10000], Loss: 0.8754\n",
      "Epoch [3860/10000], Loss: 1.1770\n",
      "Epoch [3870/10000], Loss: 1.2303\n",
      "Epoch [3880/10000], Loss: 1.1502\n",
      "Epoch [3890/10000], Loss: 1.3179\n",
      "Epoch [3900/10000], Loss: 1.3789\n",
      "Epoch [3910/10000], Loss: 0.9076\n",
      "Epoch [3920/10000], Loss: 1.4240\n",
      "Epoch [3930/10000], Loss: 1.1553\n",
      "Epoch [3940/10000], Loss: 0.8762\n",
      "Epoch [3950/10000], Loss: 1.1764\n",
      "Epoch [3960/10000], Loss: 0.9930\n",
      "Epoch [3970/10000], Loss: 1.1978\n",
      "Epoch [3980/10000], Loss: 1.3820\n",
      "Epoch [3990/10000], Loss: 1.0336\n",
      "Epoch [4000/10000], Loss: 0.7023\n",
      "Epoch [4010/10000], Loss: 0.8937\n",
      "Epoch [4020/10000], Loss: 1.2612\n",
      "Epoch [4030/10000], Loss: 0.9334\n",
      "Epoch [4040/10000], Loss: 0.9086\n",
      "Epoch [4050/10000], Loss: 0.9658\n",
      "Epoch [4060/10000], Loss: 1.2100\n",
      "Epoch [4070/10000], Loss: 0.9203\n",
      "Epoch [4080/10000], Loss: 1.0917\n",
      "Epoch [4090/10000], Loss: 1.1453\n",
      "Epoch [4100/10000], Loss: 1.3744\n",
      "Epoch [4110/10000], Loss: 1.0282\n",
      "Epoch [4120/10000], Loss: 0.9266\n",
      "Epoch [4130/10000], Loss: 0.7216\n",
      "Epoch [4140/10000], Loss: 1.4051\n",
      "Epoch [4150/10000], Loss: 1.2545\n",
      "Epoch [4160/10000], Loss: 1.2177\n",
      "Epoch [4170/10000], Loss: 1.0105\n",
      "Epoch [4180/10000], Loss: 0.9645\n",
      "Epoch [4190/10000], Loss: 1.0748\n",
      "Epoch [4200/10000], Loss: 1.0258\n",
      "Epoch [4210/10000], Loss: 1.0372\n",
      "Epoch [4220/10000], Loss: 1.3755\n",
      "Epoch [4230/10000], Loss: 0.8839\n",
      "Epoch [4240/10000], Loss: 1.5207\n",
      "Epoch [4250/10000], Loss: 1.0319\n",
      "Epoch [4260/10000], Loss: 1.0681\n",
      "Epoch [4270/10000], Loss: 1.2852\n",
      "Epoch [4280/10000], Loss: 1.0065\n",
      "Epoch [4290/10000], Loss: 1.3116\n",
      "Epoch [4300/10000], Loss: 0.8322\n",
      "Epoch [4310/10000], Loss: 0.9141\n",
      "Epoch [4320/10000], Loss: 0.9269\n",
      "Epoch [4330/10000], Loss: 1.5588\n",
      "Epoch [4340/10000], Loss: 0.8877\n",
      "Epoch [4350/10000], Loss: 1.1169\n",
      "Epoch [4360/10000], Loss: 0.8187\n",
      "Epoch [4370/10000], Loss: 1.2652\n",
      "Epoch [4380/10000], Loss: 1.2065\n",
      "Epoch [4390/10000], Loss: 1.3550\n",
      "Epoch [4400/10000], Loss: 0.7705\n",
      "Epoch [4410/10000], Loss: 0.8363\n",
      "Epoch [4420/10000], Loss: 0.7956\n",
      "Epoch [4430/10000], Loss: 0.8825\n",
      "Epoch [4440/10000], Loss: 0.7946\n",
      "Epoch [4450/10000], Loss: 0.8505\n",
      "Epoch [4460/10000], Loss: 0.9091\n",
      "Epoch [4470/10000], Loss: 1.2307\n",
      "Epoch [4480/10000], Loss: 1.1915\n",
      "Epoch [4490/10000], Loss: 1.1782\n",
      "Epoch [4500/10000], Loss: 1.3345\n",
      "Epoch [4510/10000], Loss: 1.0273\n",
      "Epoch [4520/10000], Loss: 0.7892\n",
      "Epoch [4530/10000], Loss: 1.4909\n",
      "Epoch [4540/10000], Loss: 0.8821\n",
      "Epoch [4550/10000], Loss: 0.9624\n",
      "Epoch [4560/10000], Loss: 1.5167\n",
      "Epoch [4570/10000], Loss: 0.9582\n",
      "Epoch [4580/10000], Loss: 1.1663\n",
      "Epoch [4590/10000], Loss: 0.8482\n",
      "Epoch [4600/10000], Loss: 1.1933\n",
      "Epoch [4610/10000], Loss: 1.4797\n",
      "Epoch [4620/10000], Loss: 0.7318\n",
      "Epoch [4630/10000], Loss: 0.9146\n",
      "Epoch [4640/10000], Loss: 1.0325\n",
      "Epoch [4650/10000], Loss: 0.7574\n",
      "Epoch [4660/10000], Loss: 0.8630\n",
      "Epoch [4670/10000], Loss: 1.1198\n",
      "Epoch [4680/10000], Loss: 1.3098\n",
      "Epoch [4690/10000], Loss: 0.9980\n",
      "Epoch [4700/10000], Loss: 1.1342\n",
      "Epoch [4710/10000], Loss: 1.5434\n",
      "Epoch [4720/10000], Loss: 0.9568\n",
      "Epoch [4730/10000], Loss: 1.1520\n",
      "Epoch [4740/10000], Loss: 1.0822\n",
      "Epoch [4750/10000], Loss: 0.9738\n",
      "Epoch [4760/10000], Loss: 0.8828\n",
      "Epoch [4770/10000], Loss: 0.7005\n",
      "Epoch [4780/10000], Loss: 1.5003\n",
      "Epoch [4790/10000], Loss: 0.7132\n",
      "Epoch [4800/10000], Loss: 1.3016\n",
      "Epoch [4810/10000], Loss: 0.7302\n",
      "Epoch [4820/10000], Loss: 1.2782\n",
      "Epoch [4830/10000], Loss: 0.9024\n",
      "Epoch [4840/10000], Loss: 1.3156\n",
      "Epoch [4850/10000], Loss: 1.0082\n",
      "Epoch [4860/10000], Loss: 1.2411\n",
      "Epoch [4870/10000], Loss: 0.7970\n",
      "Epoch [4880/10000], Loss: 1.3343\n",
      "Epoch [4890/10000], Loss: 0.8910\n",
      "Epoch [4900/10000], Loss: 1.3091\n",
      "Epoch [4910/10000], Loss: 1.0691\n",
      "Epoch [4920/10000], Loss: 0.8083\n",
      "Epoch [4930/10000], Loss: 1.3121\n",
      "Epoch [4940/10000], Loss: 1.3105\n",
      "Epoch [4950/10000], Loss: 0.9055\n",
      "Epoch [4960/10000], Loss: 0.9714\n",
      "Epoch [4970/10000], Loss: 1.5151\n",
      "Epoch [4980/10000], Loss: 0.6992\n",
      "Epoch [4990/10000], Loss: 1.1050\n",
      "Epoch [5000/10000], Loss: 1.0637\n",
      "Epoch [5010/10000], Loss: 0.7134\n",
      "Epoch [5020/10000], Loss: 1.3065\n",
      "Epoch [5030/10000], Loss: 0.9668\n",
      "Epoch [5040/10000], Loss: 1.2194\n",
      "Epoch [5050/10000], Loss: 0.9118\n",
      "Epoch [5060/10000], Loss: 1.2904\n",
      "Epoch [5070/10000], Loss: 1.3038\n",
      "Epoch [5080/10000], Loss: 1.1907\n",
      "Epoch [5090/10000], Loss: 0.9707\n",
      "Epoch [5100/10000], Loss: 0.8980\n",
      "Epoch [5110/10000], Loss: 1.0174\n",
      "Epoch [5120/10000], Loss: 1.1295\n",
      "Epoch [5130/10000], Loss: 1.4551\n",
      "Epoch [5140/10000], Loss: 0.7537\n",
      "Epoch [5150/10000], Loss: 0.9953\n",
      "Epoch [5160/10000], Loss: 1.2942\n",
      "Epoch [5170/10000], Loss: 1.0895\n",
      "Epoch [5180/10000], Loss: 0.8871\n",
      "Epoch [5190/10000], Loss: 1.4151\n",
      "Epoch [5200/10000], Loss: 1.1472\n",
      "Epoch [5210/10000], Loss: 0.9683\n",
      "Epoch [5220/10000], Loss: 1.2521\n",
      "Epoch [5230/10000], Loss: 1.5208\n",
      "Epoch [5240/10000], Loss: 1.2795\n",
      "Epoch [5250/10000], Loss: 1.0785\n",
      "Epoch [5260/10000], Loss: 0.8648\n",
      "Epoch [5270/10000], Loss: 1.0506\n",
      "Epoch [5280/10000], Loss: 1.4481\n",
      "Epoch [5290/10000], Loss: 0.9751\n",
      "Epoch [5300/10000], Loss: 1.2722\n",
      "Epoch [5310/10000], Loss: 0.8225\n",
      "Epoch [5320/10000], Loss: 0.9864\n",
      "Epoch [5330/10000], Loss: 1.0041\n",
      "Epoch [5340/10000], Loss: 1.2838\n",
      "Epoch [5350/10000], Loss: 0.9861\n",
      "Epoch [5360/10000], Loss: 1.4355\n",
      "Epoch [5370/10000], Loss: 1.2033\n",
      "Epoch [5380/10000], Loss: 0.8876\n",
      "Epoch [5390/10000], Loss: 1.0935\n",
      "Epoch [5400/10000], Loss: 1.1156\n",
      "Epoch [5410/10000], Loss: 1.2761\n",
      "Epoch [5420/10000], Loss: 1.1671\n",
      "Epoch [5430/10000], Loss: 1.2518\n",
      "Epoch [5440/10000], Loss: 1.2954\n",
      "Epoch [5450/10000], Loss: 0.8414\n",
      "Epoch [5460/10000], Loss: 0.8889\n",
      "Epoch [5470/10000], Loss: 0.8706\n",
      "Epoch [5480/10000], Loss: 1.1025\n",
      "Epoch [5490/10000], Loss: 0.8351\n",
      "Epoch [5500/10000], Loss: 1.2594\n",
      "Epoch [5510/10000], Loss: 1.2355\n",
      "Epoch [5520/10000], Loss: 1.3991\n",
      "Epoch [5530/10000], Loss: 1.4964\n",
      "Epoch [5540/10000], Loss: 0.7201\n",
      "Epoch [5550/10000], Loss: 0.9131\n",
      "Epoch [5560/10000], Loss: 0.9666\n",
      "Epoch [5570/10000], Loss: 1.1276\n",
      "Epoch [5580/10000], Loss: 0.7310\n",
      "Epoch [5590/10000], Loss: 0.8264\n",
      "Epoch [5600/10000], Loss: 1.5012\n",
      "Epoch [5610/10000], Loss: 0.7663\n",
      "Epoch [5620/10000], Loss: 1.0947\n",
      "Epoch [5630/10000], Loss: 1.1508\n",
      "Epoch [5640/10000], Loss: 0.8386\n",
      "Epoch [5650/10000], Loss: 0.9046\n",
      "Epoch [5660/10000], Loss: 1.6069\n",
      "Epoch [5670/10000], Loss: 1.2307\n",
      "Epoch [5680/10000], Loss: 0.8462\n",
      "Epoch [5690/10000], Loss: 1.4349\n",
      "Epoch [5700/10000], Loss: 0.8432\n",
      "Epoch [5710/10000], Loss: 0.8683\n",
      "Epoch [5720/10000], Loss: 1.3385\n",
      "Epoch [5730/10000], Loss: 1.1400\n",
      "Epoch [5740/10000], Loss: 1.0421\n",
      "Epoch [5750/10000], Loss: 1.2100\n",
      "Epoch [5760/10000], Loss: 1.0947\n",
      "Epoch [5770/10000], Loss: 0.8302\n",
      "Epoch [5780/10000], Loss: 1.2594\n",
      "Epoch [5790/10000], Loss: 0.8706\n",
      "Epoch [5800/10000], Loss: 1.3032\n",
      "Epoch [5810/10000], Loss: 1.0367\n",
      "Epoch [5820/10000], Loss: 1.0661\n",
      "Epoch [5830/10000], Loss: 1.0459\n",
      "Epoch [5840/10000], Loss: 1.4591\n",
      "Epoch [5850/10000], Loss: 1.0470\n",
      "Epoch [5860/10000], Loss: 1.5368\n",
      "Epoch [5870/10000], Loss: 1.2803\n",
      "Epoch [5880/10000], Loss: 0.8880\n",
      "Epoch [5890/10000], Loss: 0.8948\n",
      "Epoch [5900/10000], Loss: 1.1418\n",
      "Epoch [5910/10000], Loss: 1.0698\n",
      "Epoch [5920/10000], Loss: 1.1881\n",
      "Epoch [5930/10000], Loss: 0.9990\n",
      "Epoch [5940/10000], Loss: 1.1764\n",
      "Epoch [5950/10000], Loss: 0.8459\n",
      "Epoch [5960/10000], Loss: 1.5187\n",
      "Epoch [5970/10000], Loss: 0.8754\n",
      "Epoch [5980/10000], Loss: 0.9800\n",
      "Epoch [5990/10000], Loss: 1.4378\n",
      "Epoch [6000/10000], Loss: 0.7688\n",
      "Epoch [6010/10000], Loss: 1.3512\n",
      "Epoch [6020/10000], Loss: 1.1032\n",
      "Epoch [6030/10000], Loss: 0.8631\n",
      "Epoch [6040/10000], Loss: 0.7857\n",
      "Epoch [6050/10000], Loss: 1.1797\n",
      "Epoch [6060/10000], Loss: 0.9564\n",
      "Epoch [6070/10000], Loss: 1.3360\n",
      "Epoch [6080/10000], Loss: 1.3919\n",
      "Epoch [6090/10000], Loss: 1.1468\n",
      "Epoch [6100/10000], Loss: 1.1105\n",
      "Epoch [6110/10000], Loss: 0.7047\n",
      "Epoch [6120/10000], Loss: 1.1080\n",
      "Epoch [6130/10000], Loss: 0.7900\n",
      "Epoch [6140/10000], Loss: 0.8040\n",
      "Epoch [6150/10000], Loss: 1.3855\n",
      "Epoch [6160/10000], Loss: 0.9109\n",
      "Epoch [6170/10000], Loss: 1.1146\n",
      "Epoch [6180/10000], Loss: 1.3807\n",
      "Epoch [6190/10000], Loss: 0.9564\n",
      "Epoch [6200/10000], Loss: 1.2472\n",
      "Epoch [6210/10000], Loss: 0.6954\n",
      "Epoch [6220/10000], Loss: 0.9841\n",
      "Epoch [6230/10000], Loss: 0.9317\n",
      "Epoch [6240/10000], Loss: 0.9367\n",
      "Epoch [6250/10000], Loss: 1.2614\n",
      "Epoch [6260/10000], Loss: 0.8940\n",
      "Epoch [6270/10000], Loss: 1.0322\n",
      "Epoch [6280/10000], Loss: 1.2859\n",
      "Epoch [6290/10000], Loss: 1.0776\n",
      "Epoch [6300/10000], Loss: 1.2987\n",
      "Epoch [6310/10000], Loss: 0.8394\n",
      "Epoch [6320/10000], Loss: 0.9627\n",
      "Epoch [6330/10000], Loss: 0.7739\n",
      "Epoch [6340/10000], Loss: 0.8113\n",
      "Epoch [6350/10000], Loss: 0.9885\n",
      "Epoch [6360/10000], Loss: 1.3117\n",
      "Epoch [6370/10000], Loss: 1.1255\n",
      "Epoch [6380/10000], Loss: 1.1198\n",
      "Epoch [6390/10000], Loss: 0.8034\n",
      "Epoch [6400/10000], Loss: 0.7663\n",
      "Epoch [6410/10000], Loss: 1.3531\n",
      "Epoch [6420/10000], Loss: 1.2927\n",
      "Epoch [6430/10000], Loss: 0.8377\n",
      "Epoch [6440/10000], Loss: 1.2292\n",
      "Epoch [6450/10000], Loss: 0.8136\n",
      "Epoch [6460/10000], Loss: 0.9061\n",
      "Epoch [6470/10000], Loss: 0.7622\n",
      "Epoch [6480/10000], Loss: 1.1283\n",
      "Epoch [6490/10000], Loss: 1.1849\n",
      "Epoch [6500/10000], Loss: 1.1093\n",
      "Epoch [6510/10000], Loss: 1.3950\n",
      "Epoch [6520/10000], Loss: 0.7916\n",
      "Epoch [6530/10000], Loss: 0.7505\n",
      "Epoch [6540/10000], Loss: 1.1105\n",
      "Epoch [6550/10000], Loss: 1.3013\n",
      "Epoch [6560/10000], Loss: 0.8784\n",
      "Epoch [6570/10000], Loss: 1.0841\n",
      "Epoch [6580/10000], Loss: 0.8741\n",
      "Epoch [6590/10000], Loss: 1.4945\n",
      "Epoch [6600/10000], Loss: 1.2602\n",
      "Epoch [6610/10000], Loss: 1.2439\n",
      "Epoch [6620/10000], Loss: 1.0525\n",
      "Epoch [6630/10000], Loss: 1.4233\n",
      "Epoch [6640/10000], Loss: 1.1641\n",
      "Epoch [6650/10000], Loss: 0.7859\n",
      "Epoch [6660/10000], Loss: 1.0762\n",
      "Epoch [6670/10000], Loss: 0.9411\n",
      "Epoch [6680/10000], Loss: 1.1586\n",
      "Epoch [6690/10000], Loss: 1.2714\n",
      "Epoch [6700/10000], Loss: 0.8432\n",
      "Epoch [6710/10000], Loss: 0.8284\n",
      "Epoch [6720/10000], Loss: 1.3142\n",
      "Epoch [6730/10000], Loss: 1.4991\n",
      "Epoch [6740/10000], Loss: 0.9534\n",
      "Epoch [6750/10000], Loss: 0.9151\n",
      "Epoch [6760/10000], Loss: 1.2111\n",
      "Epoch [6770/10000], Loss: 0.9480\n",
      "Epoch [6780/10000], Loss: 0.8689\n",
      "Epoch [6790/10000], Loss: 0.7882\n",
      "Epoch [6800/10000], Loss: 1.1304\n",
      "Epoch [6810/10000], Loss: 1.1981\n",
      "Epoch [6820/10000], Loss: 1.1454\n",
      "Epoch [6830/10000], Loss: 0.8986\n",
      "Epoch [6840/10000], Loss: 1.0307\n",
      "Epoch [6850/10000], Loss: 0.9432\n",
      "Epoch [6860/10000], Loss: 1.1340\n",
      "Epoch [6870/10000], Loss: 1.4148\n",
      "Epoch [6880/10000], Loss: 0.8314\n",
      "Epoch [6890/10000], Loss: 1.0336\n",
      "Epoch [6900/10000], Loss: 0.8218\n",
      "Epoch [6910/10000], Loss: 0.8410\n",
      "Epoch [6920/10000], Loss: 1.4726\n",
      "Epoch [6930/10000], Loss: 1.5169\n",
      "Epoch [6940/10000], Loss: 1.3518\n",
      "Epoch [6950/10000], Loss: 0.7547\n",
      "Epoch [6960/10000], Loss: 0.9903\n",
      "Epoch [6970/10000], Loss: 0.9561\n",
      "Epoch [6980/10000], Loss: 1.3116\n",
      "Epoch [6990/10000], Loss: 1.1997\n",
      "Epoch [7000/10000], Loss: 0.7132\n",
      "Epoch [7010/10000], Loss: 0.8654\n",
      "Epoch [7020/10000], Loss: 0.6381\n",
      "Epoch [7030/10000], Loss: 0.8991\n",
      "Epoch [7040/10000], Loss: 1.4861\n",
      "Epoch [7050/10000], Loss: 0.8410\n",
      "Epoch [7060/10000], Loss: 1.2920\n",
      "Epoch [7070/10000], Loss: 0.7634\n",
      "Epoch [7080/10000], Loss: 1.3375\n",
      "Epoch [7090/10000], Loss: 0.8134\n",
      "Epoch [7100/10000], Loss: 0.7398\n",
      "Epoch [7110/10000], Loss: 0.8796\n",
      "Epoch [7120/10000], Loss: 0.9855\n",
      "Epoch [7130/10000], Loss: 1.3513\n",
      "Epoch [7140/10000], Loss: 1.3991\n",
      "Epoch [7150/10000], Loss: 0.7258\n",
      "Epoch [7160/10000], Loss: 1.3796\n",
      "Epoch [7170/10000], Loss: 0.9632\n",
      "Epoch [7180/10000], Loss: 0.9065\n",
      "Epoch [7190/10000], Loss: 0.9897\n",
      "Epoch [7200/10000], Loss: 1.0453\n",
      "Epoch [7210/10000], Loss: 1.0196\n",
      "Epoch [7220/10000], Loss: 0.6779\n",
      "Epoch [7230/10000], Loss: 0.7681\n",
      "Epoch [7240/10000], Loss: 1.2307\n",
      "Epoch [7250/10000], Loss: 1.2980\n",
      "Epoch [7260/10000], Loss: 1.1463\n",
      "Epoch [7270/10000], Loss: 0.9380\n",
      "Epoch [7280/10000], Loss: 0.8698\n",
      "Epoch [7290/10000], Loss: 0.6888\n",
      "Epoch [7300/10000], Loss: 0.9640\n",
      "Epoch [7310/10000], Loss: 0.9621\n",
      "Epoch [7320/10000], Loss: 1.0008\n",
      "Epoch [7330/10000], Loss: 0.6993\n",
      "Epoch [7340/10000], Loss: 0.6740\n",
      "Epoch [7350/10000], Loss: 0.9087\n",
      "Epoch [7360/10000], Loss: 0.9837\n",
      "Epoch [7370/10000], Loss: 1.2165\n",
      "Epoch [7380/10000], Loss: 1.2767\n",
      "Epoch [7390/10000], Loss: 0.8417\n",
      "Epoch [7400/10000], Loss: 0.8315\n",
      "Epoch [7410/10000], Loss: 0.8637\n",
      "Epoch [7420/10000], Loss: 1.3744\n",
      "Epoch [7430/10000], Loss: 0.9664\n",
      "Epoch [7440/10000], Loss: 1.2539\n",
      "Epoch [7450/10000], Loss: 1.0223\n",
      "Epoch [7460/10000], Loss: 1.2055\n",
      "Epoch [7470/10000], Loss: 1.0965\n",
      "Epoch [7480/10000], Loss: 0.9106\n",
      "Epoch [7490/10000], Loss: 0.8695\n",
      "Epoch [7500/10000], Loss: 0.8823\n",
      "Epoch [7510/10000], Loss: 0.7779\n",
      "Epoch [7520/10000], Loss: 1.2359\n",
      "Epoch [7530/10000], Loss: 1.0902\n",
      "Epoch [7540/10000], Loss: 0.9760\n",
      "Epoch [7550/10000], Loss: 1.1993\n",
      "Epoch [7560/10000], Loss: 0.8321\n",
      "Epoch [7570/10000], Loss: 1.2042\n",
      "Epoch [7580/10000], Loss: 1.2815\n",
      "Epoch [7590/10000], Loss: 1.2541\n",
      "Epoch [7600/10000], Loss: 1.0367\n",
      "Epoch [7610/10000], Loss: 1.3820\n",
      "Epoch [7620/10000], Loss: 0.7754\n",
      "Epoch [7630/10000], Loss: 0.8839\n",
      "Epoch [7640/10000], Loss: 1.2983\n",
      "Epoch [7650/10000], Loss: 1.2125\n",
      "Epoch [7660/10000], Loss: 0.8943\n",
      "Epoch [7670/10000], Loss: 1.1014\n",
      "Epoch [7680/10000], Loss: 0.8229\n",
      "Epoch [7690/10000], Loss: 0.7824\n",
      "Epoch [7700/10000], Loss: 1.1073\n",
      "Epoch [7710/10000], Loss: 0.8976\n",
      "Epoch [7720/10000], Loss: 1.5385\n",
      "Epoch [7730/10000], Loss: 1.0268\n",
      "Epoch [7740/10000], Loss: 1.3777\n",
      "Epoch [7750/10000], Loss: 1.1996\n",
      "Epoch [7760/10000], Loss: 0.8635\n",
      "Epoch [7770/10000], Loss: 1.1801\n",
      "Epoch [7780/10000], Loss: 1.2343\n",
      "Epoch [7790/10000], Loss: 1.3538\n",
      "Epoch [7800/10000], Loss: 0.8411\n",
      "Epoch [7810/10000], Loss: 1.1389\n",
      "Epoch [7820/10000], Loss: 1.2606\n",
      "Epoch [7830/10000], Loss: 0.8718\n",
      "Epoch [7840/10000], Loss: 0.8052\n",
      "Epoch [7850/10000], Loss: 0.9830\n",
      "Epoch [7860/10000], Loss: 1.0042\n",
      "Epoch [7870/10000], Loss: 0.8113\n",
      "Epoch [7880/10000], Loss: 0.7967\n",
      "Epoch [7890/10000], Loss: 1.3550\n",
      "Epoch [7900/10000], Loss: 0.7648\n",
      "Epoch [7910/10000], Loss: 1.5016\n",
      "Epoch [7920/10000], Loss: 1.5038\n",
      "Epoch [7930/10000], Loss: 0.8445\n",
      "Epoch [7940/10000], Loss: 0.8300\n",
      "Epoch [7950/10000], Loss: 1.0039\n",
      "Epoch [7960/10000], Loss: 1.4662\n",
      "Epoch [7970/10000], Loss: 1.2737\n",
      "Epoch [7980/10000], Loss: 1.0457\n",
      "Epoch [7990/10000], Loss: 1.3819\n",
      "Epoch [8000/10000], Loss: 0.8770\n",
      "Epoch [8010/10000], Loss: 1.0105\n",
      "Epoch [8020/10000], Loss: 0.9116\n",
      "Epoch [8030/10000], Loss: 0.7989\n",
      "Epoch [8040/10000], Loss: 0.9213\n",
      "Epoch [8050/10000], Loss: 0.9383\n",
      "Epoch [8060/10000], Loss: 1.2306\n",
      "Epoch [8070/10000], Loss: 0.9823\n",
      "Epoch [8080/10000], Loss: 1.1451\n",
      "Epoch [8090/10000], Loss: 1.0766\n",
      "Epoch [8100/10000], Loss: 0.8149\n",
      "Epoch [8110/10000], Loss: 1.3378\n",
      "Epoch [8120/10000], Loss: 0.9523\n",
      "Epoch [8130/10000], Loss: 1.5449\n",
      "Epoch [8140/10000], Loss: 0.8230\n",
      "Epoch [8150/10000], Loss: 0.8812\n",
      "Epoch [8160/10000], Loss: 0.9196\n",
      "Epoch [8170/10000], Loss: 0.9882\n",
      "Epoch [8180/10000], Loss: 1.2722\n",
      "Epoch [8190/10000], Loss: 1.1276\n",
      "Epoch [8200/10000], Loss: 0.8780\n",
      "Epoch [8210/10000], Loss: 0.9404\n",
      "Epoch [8220/10000], Loss: 0.8586\n",
      "Epoch [8230/10000], Loss: 0.9994\n",
      "Epoch [8240/10000], Loss: 1.2523\n",
      "Epoch [8250/10000], Loss: 1.5410\n",
      "Epoch [8260/10000], Loss: 1.0731\n",
      "Epoch [8270/10000], Loss: 1.0636\n",
      "Epoch [8280/10000], Loss: 1.1035\n",
      "Epoch [8290/10000], Loss: 0.9028\n",
      "Epoch [8300/10000], Loss: 1.2482\n",
      "Epoch [8310/10000], Loss: 1.1406\n",
      "Epoch [8320/10000], Loss: 0.7157\n",
      "Epoch [8330/10000], Loss: 0.8166\n",
      "Epoch [8340/10000], Loss: 1.1074\n",
      "Epoch [8350/10000], Loss: 1.1080\n",
      "Epoch [8360/10000], Loss: 1.4367\n",
      "Epoch [8370/10000], Loss: 1.3228\n",
      "Epoch [8380/10000], Loss: 1.4125\n",
      "Epoch [8390/10000], Loss: 0.9779\n",
      "Epoch [8400/10000], Loss: 0.9273\n",
      "Epoch [8410/10000], Loss: 1.2784\n",
      "Epoch [8420/10000], Loss: 1.4591\n",
      "Epoch [8430/10000], Loss: 0.9449\n",
      "Epoch [8440/10000], Loss: 1.0281\n",
      "Epoch [8450/10000], Loss: 1.0948\n",
      "Epoch [8460/10000], Loss: 0.8056\n",
      "Epoch [8470/10000], Loss: 0.8284\n",
      "Epoch [8480/10000], Loss: 1.2156\n",
      "Epoch [8490/10000], Loss: 1.0343\n",
      "Epoch [8500/10000], Loss: 0.7106\n",
      "Epoch [8510/10000], Loss: 1.1610\n",
      "Epoch [8520/10000], Loss: 1.3486\n",
      "Epoch [8530/10000], Loss: 1.2923\n",
      "Epoch [8540/10000], Loss: 0.9449\n",
      "Epoch [8550/10000], Loss: 1.1035\n",
      "Epoch [8560/10000], Loss: 1.5012\n",
      "Epoch [8570/10000], Loss: 0.8751\n",
      "Epoch [8580/10000], Loss: 1.0457\n",
      "Epoch [8590/10000], Loss: 1.2322\n",
      "Epoch [8600/10000], Loss: 1.3098\n",
      "Epoch [8610/10000], Loss: 0.8409\n",
      "Epoch [8620/10000], Loss: 1.5431\n",
      "Epoch [8630/10000], Loss: 0.8874\n",
      "Epoch [8640/10000], Loss: 1.4970\n",
      "Epoch [8650/10000], Loss: 0.8828\n",
      "Epoch [8660/10000], Loss: 1.1454\n",
      "Epoch [8670/10000], Loss: 0.9480\n",
      "Epoch [8680/10000], Loss: 1.4576\n",
      "Epoch [8690/10000], Loss: 1.0678\n",
      "Epoch [8700/10000], Loss: 1.1848\n",
      "Epoch [8710/10000], Loss: 1.3393\n",
      "Epoch [8720/10000], Loss: 1.1553\n",
      "Epoch [8730/10000], Loss: 1.2736\n",
      "Epoch [8740/10000], Loss: 1.2276\n",
      "Epoch [8750/10000], Loss: 0.7788\n",
      "Epoch [8760/10000], Loss: 1.0285\n",
      "Epoch [8770/10000], Loss: 1.1499\n",
      "Epoch [8780/10000], Loss: 1.0108\n",
      "Epoch [8790/10000], Loss: 1.0762\n",
      "Epoch [8800/10000], Loss: 1.0078\n",
      "Epoch [8810/10000], Loss: 1.2680\n",
      "Epoch [8820/10000], Loss: 0.8678\n",
      "Epoch [8830/10000], Loss: 1.3289\n",
      "Epoch [8840/10000], Loss: 0.8673\n",
      "Epoch [8850/10000], Loss: 0.9843\n",
      "Epoch [8860/10000], Loss: 0.8546\n",
      "Epoch [8870/10000], Loss: 0.9967\n",
      "Epoch [8880/10000], Loss: 1.3912\n",
      "Epoch [8890/10000], Loss: 1.5169\n",
      "Epoch [8900/10000], Loss: 1.1230\n",
      "Epoch [8910/10000], Loss: 1.4906\n",
      "Epoch [8920/10000], Loss: 0.8852\n",
      "Epoch [8930/10000], Loss: 0.7590\n",
      "Epoch [8940/10000], Loss: 0.8082\n",
      "Epoch [8950/10000], Loss: 1.0691\n",
      "Epoch [8960/10000], Loss: 1.2397\n",
      "Epoch [8970/10000], Loss: 1.4367\n",
      "Epoch [8980/10000], Loss: 0.8200\n",
      "Epoch [8990/10000], Loss: 0.8441\n",
      "Epoch [9000/10000], Loss: 0.8567\n",
      "Epoch [9010/10000], Loss: 0.8796\n",
      "Epoch [9020/10000], Loss: 0.9659\n",
      "Epoch [9030/10000], Loss: 0.9646\n",
      "Epoch [9040/10000], Loss: 1.3168\n",
      "Epoch [9050/10000], Loss: 1.0679\n",
      "Epoch [9060/10000], Loss: 1.2230\n",
      "Epoch [9070/10000], Loss: 1.1169\n",
      "Epoch [9080/10000], Loss: 0.6956\n",
      "Epoch [9090/10000], Loss: 1.1635\n",
      "Epoch [9100/10000], Loss: 1.2747\n",
      "Epoch [9110/10000], Loss: 0.7923\n",
      "Epoch [9120/10000], Loss: 1.0716\n",
      "Epoch [9130/10000], Loss: 1.1073\n",
      "Epoch [9140/10000], Loss: 1.5184\n",
      "Epoch [9150/10000], Loss: 1.2658\n",
      "Epoch [9160/10000], Loss: 1.0896\n",
      "Epoch [9170/10000], Loss: 0.9778\n",
      "Epoch [9180/10000], Loss: 1.2654\n",
      "Epoch [9190/10000], Loss: 1.1598\n",
      "Epoch [9200/10000], Loss: 1.2317\n",
      "Epoch [9210/10000], Loss: 0.7014\n",
      "Epoch [9220/10000], Loss: 1.2034\n",
      "Epoch [9230/10000], Loss: 1.0107\n",
      "Epoch [9240/10000], Loss: 1.0665\n",
      "Epoch [9250/10000], Loss: 1.1617\n",
      "Epoch [9260/10000], Loss: 0.7198\n",
      "Epoch [9270/10000], Loss: 0.8777\n",
      "Epoch [9280/10000], Loss: 1.2966\n",
      "Epoch [9290/10000], Loss: 0.8655\n",
      "Epoch [9300/10000], Loss: 1.2717\n",
      "Epoch [9310/10000], Loss: 0.8765\n",
      "Epoch [9320/10000], Loss: 1.3486\n",
      "Epoch [9330/10000], Loss: 0.7659\n",
      "Epoch [9340/10000], Loss: 0.7718\n",
      "Epoch [9350/10000], Loss: 0.8444\n",
      "Epoch [9360/10000], Loss: 0.6779\n",
      "Epoch [9370/10000], Loss: 0.7795\n",
      "Epoch [9380/10000], Loss: 1.2113\n",
      "Epoch [9390/10000], Loss: 0.9035\n",
      "Epoch [9400/10000], Loss: 1.2914\n",
      "Epoch [9410/10000], Loss: 0.8311\n",
      "Epoch [9420/10000], Loss: 1.3308\n",
      "Epoch [9430/10000], Loss: 0.9052\n",
      "Epoch [9440/10000], Loss: 1.1827\n",
      "Epoch [9450/10000], Loss: 0.9201\n",
      "Epoch [9460/10000], Loss: 0.8832\n",
      "Epoch [9470/10000], Loss: 0.9800\n",
      "Epoch [9480/10000], Loss: 0.7833\n",
      "Epoch [9490/10000], Loss: 1.3556\n",
      "Epoch [9500/10000], Loss: 0.6950\n",
      "Epoch [9510/10000], Loss: 0.6937\n",
      "Epoch [9520/10000], Loss: 0.9411\n",
      "Epoch [9530/10000], Loss: 0.9903\n",
      "Epoch [9540/10000], Loss: 1.0349\n",
      "Epoch [9550/10000], Loss: 0.8784\n",
      "Epoch [9560/10000], Loss: 0.8965\n",
      "Epoch [9570/10000], Loss: 1.3224\n",
      "Epoch [9580/10000], Loss: 0.8555\n",
      "Epoch [9590/10000], Loss: 1.4431\n",
      "Epoch [9600/10000], Loss: 0.7788\n",
      "Epoch [9610/10000], Loss: 1.4491\n",
      "Epoch [9620/10000], Loss: 0.7372\n",
      "Epoch [9630/10000], Loss: 0.8564\n",
      "Epoch [9640/10000], Loss: 1.4809\n",
      "Epoch [9650/10000], Loss: 0.9601\n",
      "Epoch [9660/10000], Loss: 0.6947\n",
      "Epoch [9670/10000], Loss: 0.7176\n",
      "Epoch [9680/10000], Loss: 1.1147\n",
      "Epoch [9690/10000], Loss: 1.1010\n",
      "Epoch [9700/10000], Loss: 1.3315\n",
      "Epoch [9710/10000], Loss: 0.7379\n",
      "Epoch [9720/10000], Loss: 0.6779\n",
      "Epoch [9730/10000], Loss: 1.3453\n",
      "Epoch [9740/10000], Loss: 0.7545\n",
      "Epoch [9750/10000], Loss: 0.8884\n",
      "Epoch [9760/10000], Loss: 1.1719\n",
      "Epoch [9770/10000], Loss: 0.8943\n",
      "Epoch [9780/10000], Loss: 0.9133\n",
      "Epoch [9790/10000], Loss: 1.1354\n",
      "Epoch [9800/10000], Loss: 1.2606\n",
      "Epoch [9810/10000], Loss: 1.2307\n",
      "Epoch [9820/10000], Loss: 0.8260\n",
      "Epoch [9830/10000], Loss: 0.7748\n",
      "Epoch [9840/10000], Loss: 0.8937\n",
      "Epoch [9850/10000], Loss: 0.8681\n",
      "Epoch [9860/10000], Loss: 1.1634\n",
      "Epoch [9870/10000], Loss: 1.3486\n",
      "Epoch [9880/10000], Loss: 1.1475\n",
      "Epoch [9890/10000], Loss: 1.3538\n",
      "Epoch [9900/10000], Loss: 1.0662\n",
      "Epoch [9910/10000], Loss: 1.0857\n",
      "Epoch [9920/10000], Loss: 1.1523\n",
      "Epoch [9930/10000], Loss: 1.2836\n",
      "Epoch [9940/10000], Loss: 1.5184\n",
      "Epoch [9950/10000], Loss: 1.3113\n",
      "Epoch [9960/10000], Loss: 0.8648\n",
      "Epoch [9970/10000], Loss: 0.9634\n",
      "Epoch [9980/10000], Loss: 1.5217\n",
      "Epoch [9990/10000], Loss: 1.1978\n",
      "Epoch [10000/10000], Loss: 1.5629\n"
     ]
    }
   ],
   "source": [
    "fit(10000,model,loss_fn,opt,train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1.1853,  65.3137],\n",
       "         [ -1.6425,  82.6146],\n",
       "         [  2.3454, 101.4529],\n",
       "         [  6.4574,  70.1655],\n",
       "         [ -6.7086,  74.2826],\n",
       "         [  1.2888,  65.3898],\n",
       "         [ -2.0305,  82.0993],\n",
       "         [  2.1761, 101.9054],\n",
       "         [  6.3539,  70.0894],\n",
       "         [ -7.2001,  73.6913],\n",
       "         [  0.7973,  64.7984],\n",
       "         [ -1.5390,  82.6907],\n",
       "         [  2.7334, 101.9681],\n",
       "         [  6.9489,  70.7568],\n",
       "         [ -6.8121,  74.2065]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.],\n",
       "         [ 57.,  69.],\n",
       "         [ 80., 102.],\n",
       "         [118., 132.],\n",
       "         [ 21.,  38.],\n",
       "         [104., 118.],\n",
       "         [ 57.,  69.],\n",
       "         [ 82., 100.],\n",
       "         [118., 134.],\n",
       "         [ 20.,  38.],\n",
       "         [102., 120.]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
